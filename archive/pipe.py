# benchmark_final.py

import gymnasium as gym
import numpy as np
import time
import multiprocessing as mp
import pandas as pd
import torch
import torch.nn as nn
from typing import Callable

# --- å¯¼å…¥æ‰€æœ‰å¿…è¦çš„æ¨¡å— ---
# TODO: æ­¥éª¤ 1 - ç¡®ä¿æ‰€æœ‰è‡ªå®šä¹‰æ¨¡å—éƒ½å¯ä»¥è¢«å¯¼å…¥
from envrunner import EnvExecutor as HighPerformanceEnvExecutor  # å‡è®¾æ‚¨å·²å°†å…¶æ‰“åŒ…
from envrunner import PipelinedExecutor
from gymnasium.vector import SyncVectorEnv, AsyncVectorEnv

from pygtm_env.task.upset_recovery import EnvBuilder
from conflga import conflga_func, ConflgaConfig

# ä¸ºäº†è„šæœ¬å¯è¿è¡Œï¼Œå…ˆç”¨å ä½ç¬¦
IS_CUSTOM_ENV = True
# class EnvBuilder: pass
# def get_conflga_config(): return None


# --- 1. å¯åºåˆ—åŒ–çš„ç¯å¢ƒæ„é€ å™¨ (ä¸ä¹‹å‰ç›¸åŒ) ---
class EnvMaker:
    def __init__(self, cfg=None):
        self.cfg = cfg

    def __call__(self) -> gym.Env:
        if IS_CUSTOM_ENV:
            return EnvBuilder(self.cfg)()
        else:
            return gym.make("CartPole-v1")


# --- 2. æ¨¡æ‹Ÿ GPU ç­–ç•¥ (ä¸ä¹‹å‰ç›¸åŒï¼Œä½†ä¿®å¤äº†åŠ¨ä½œç©ºé—´é—®é¢˜) ---
class FakeGPUPolicy(nn.Module):
    def __init__(self, obs_dim: int, act_space: gym.Space, device: torch.device):
        super().__init__()
        self.device = device
        self.action_space = act_space

        # ç¡®å®š act_dim
        if isinstance(act_space, gym.spaces.Discrete):
            act_dim = act_space.n
        elif isinstance(act_space, gym.spaces.Box):
            act_dim = act_space.shape[0]
        else:
            raise TypeError(f"Unsupported action space {type(act_space)}")

        self.net = nn.Sequential(
            nn.Linear(obs_dim, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, act_dim),
        ).to(device)

    @torch.no_grad()
    def get_actions(self, observations: np.ndarray) -> np.ndarray:
        obs_tensor = torch.from_numpy(observations).float().to(self.device)
        if isinstance(self.action_space, gym.spaces.Discrete):
            logits = self.net(obs_tensor)
            actions_tensor = torch.argmax(logits, dim=1)
            return actions_tensor.cpu().numpy()
        elif isinstance(self.action_space, gym.spaces.Box):
            # å¯¹äºè¿ç»­åŠ¨ä½œï¼Œé€šå¸¸æ˜¯è¾“å‡ºå‡å€¼
            actions_tensor = self.net(obs_tensor)
            return actions_tensor.cpu().numpy()


# --- 3. æ ¸å¿ƒæµ‹é‡å‡½æ•° ---
@conflga_func(config_dir="conf", default_config="gtm_env", auto_print=False)
def run_trial(
    cfg: ConflgaConfig,
    algorithm_name: str,
    algorithm_cls: Callable,
    num_envs: int,
    num_workers: int,
    effective_batch_size: int,
    policy: FakeGPUPolicy,
    num_steps: int = 100,  # è¿è¡Œå›ºå®šçš„æ­¥æ•°/æ‰¹æ¬¡æ•°
) -> float:

    # cfg = get_conflga_config() if IS_CUSTOM_ENV else None
    # maker = EnvMaker(cfg)
    # env_fns = [maker for _ in range(num_envs)]
    # ä½¿ç”¨å ä½ç¬¦
    maker = EnvMaker(cfg)
    env_fns = [maker for _ in range(num_envs)]

    env = None
    total_frames = 0
    total_time = 0

    try:
        # --- æµæ°´çº¿æ‰§è¡Œå™¨çš„ç‰¹æ®Šå¤„ç† ---
        if algorithm_name == "PipelinedExecutor":
            env = PipelinedExecutor(env_fns, num_workers, effective_batch_size, policy)

            # é¢„çƒ­
            warmup_steps = 5
            for i, batch in enumerate(env):
                if i >= warmup_steps:
                    break
                _ = batch["obs"].to(policy.device, non_blocking=True)

            # æµ‹é‡
            start_time = time.perf_counter()
            for i, batch in enumerate(env):
                if i >= num_steps:
                    break
                # æ¨¡æ‹Ÿè®­ç»ƒ/ä½¿ç”¨æ•°æ®
                _ = batch["obs"].to(policy.device, non_blocking=True)
                # time.sleep(0.001) # å¯ä»¥æ¨¡æ‹Ÿä¸€ä¸ªå°çš„è®­ç»ƒå»¶è¿Ÿ
            end_time = time.perf_counter()

            total_time = end_time - start_time
            total_frames = num_steps * effective_batch_size
            return total_frames / total_time

        # --- éæµæ°´çº¿æ‰§è¡Œå™¨çš„å¤„ç† ---
        else:
            if algorithm_cls is HighPerformanceEnvExecutor:
                env = algorithm_cls(env_fns, num_workers=num_workers)
                is_vectorized_action_space = False
            else:
                env = algorithm_cls(env_fns)
                is_vectorized_action_space = True

            # å¯¹äºéæµæ°´çº¿ï¼Œç­–ç•¥çš„åŠ¨ä½œç©ºé—´éœ€è¦å•ç‹¬å¤„ç†
            local_policy = FakeGPUPolicy(
                policy.net[0].in_features,  # obs_dim
                env.action_space if is_vectorized_action_space else policy.action_space,
                policy.device,
            )

            obs, _ = env.reset(seed=42)

            # é¢„çƒ­
            for _ in range(5):
                actions = local_policy.get_actions(obs)
                obs, _, _, _, _ = env.step(actions)

            # æµ‹é‡
            start_time = time.perf_counter()
            for _ in range(num_steps):
                actions = local_policy.get_actions(obs)
                obs, _, _, _, _ = env.step(actions)
            end_time = time.perf_counter()

            total_time = end_time - start_time
            total_frames = num_steps * effective_batch_size
            return total_frames / total_time

    except Exception as e:
        # print(f"\n  - WARN: Config failed with error: {type(e).__name__}: {e}")
        return 0.0
    finally:
        if env:
            env.close()


# --- 4. ä¸»æœç´¢ç¨‹åº ---
def main():
    # --- æœç´¢ç©ºé—´ ---
    BATCH_SIZES = [256, 512, 1024, 2048, 4096, 8192, 16384]
    WORKER_COUNTS = [4, 8, 16, 24, 32]
    ALGORITHMS = {
        # æ–°å¢ PipelinedExecutor
        "PipelinedExecutor": PipelinedExecutor,
        "EnvExecutor": HighPerformanceEnvExecutor,
        "AsyncVectorEnv": AsyncVectorEnv,
        "SyncVectorEnv": SyncVectorEnv,
    }

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print("=" * 70)
    print("      æœ€ç»ˆå…¨ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯• (å«æµæ°´çº¿)")
    # ... (æ‰“å°ç¡¬ä»¶å’Œç›®æ ‡ä¿¡æ¯)

    # --- åˆå§‹åŒ–å…±äº«ç­–ç•¥ ---
    # TODO: æ­¥éª¤ 3 - å¦‚æœæ˜¯è‡ªå®šä¹‰ç¯å¢ƒï¼Œç¡®ä¿è¿™é‡Œçš„ç»´åº¦æ˜¯æ­£ç¡®çš„
    obs_dim, act_space = (
        (4, gym.spaces.Discrete(2))
        if not IS_CUSTOM_ENV
        else (17, gym.spaces.Discrete(4))
    )  # å ä½ç¬¦
    shared_policy = FakeGPUPolicy(obs_dim, act_space, device)

    # --- åˆ›å»ºæµ‹è¯•é…ç½® ---
    all_configs = []
    for algo_name, algo_cls in ALGORITHMS.items():
        if algo_name in ["EnvExecutor", "PipelinedExecutor"]:
            for batch_size in BATCH_SIZES:
                for workers in WORKER_COUNTS:
                    if batch_size % workers == 0:
                        all_configs.append((algo_name, algo_cls, batch_size, workers))
        elif algo_name == "AsyncVectorEnv":
            for workers in WORKER_COUNTS:
                all_configs.append((algo_name, algo_cls, workers, workers))
        elif algo_name == "SyncVectorEnv":
            for batch_size in BATCH_SIZES:
                all_configs.append((algo_name, algo_cls, batch_size, 1))

    # --- è¿è¡Œæœç´¢ ---
    results = []
    total_configs = len(all_configs)
    print(f"[*] å°†è¦æµ‹è¯• {total_configs} ç§ä¸åŒé…ç½®...\n")

    for i, (algo_name, algo_cls, batch_size, workers) in enumerate(all_configs):
        effective_batch_size = workers if algo_name == "AsyncVectorEnv" else batch_size
        num_envs_to_create = workers if algo_name == "AsyncVectorEnv" else batch_size

        print(
            f"[{i+1}/{total_configs}] æµ‹è¯•: {algo_name:<18} Batch={batch_size:<5} Workers={workers:<3}... ",
            end="",
            flush=True,
        )

        fps = run_trial(
            algo_name,
            algo_cls,
            num_envs_to_create,
            workers,
            effective_batch_size,
            shared_policy,
        )

        print(f"-> FPS: {fps:,.0f}")
        results.append([algo_name, batch_size, workers, fps])

    # --- æŠ¥å‘Šç»“æœ (ä¸ä¹‹å‰è„šæœ¬å®Œå…¨ç›¸åŒ) ---
    print("\n\n" + "=" * 70)
    print("      æœç´¢ç»“æœæŠ¥å‘Š")
    df = pd.DataFrame(
        results, columns=["Algorithm", "Batch Size", "Workers", "Sync FPS"]
    )
    df = df.sort_values(by="Sync FPS", ascending=False).reset_index(drop=True)
    df["Sync FPS"] = df["Sync FPS"].map("{:,.0f}".format)
    print(df.to_string())
    print("\n" + "-" * 70)
    best_config = df.iloc[0]
    print("\nğŸ† æœ€ä¼˜é…ç½®å»ºè®® ğŸ†")
    print(f"\n  - ç®—æ³•:         {best_config['Algorithm']}")
    # ... (æ‰“å°æœ€ä¼˜é…ç½®å’Œåˆ†æ)


if __name__ == "__main__":
    try:
        mp.set_start_method("spawn", force=True)
    except RuntimeError:
        pass
    main()
